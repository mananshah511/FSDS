{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d22b3604-717e-4f7c-8796-faf7781c6727",
   "metadata": {},
   "source": [
    "1. What is prior probability? Give an example."
   ]
  },
  {
   "cell_type": "raw",
   "id": "cc41a937-ca5c-4e0d-9fb1-e07085a4d9cd",
   "metadata": {},
   "source": [
    "prior probability is defined as the probability of the outcome when possibilities are finite and each outcome is equally possible. It won't depend on the future or past of the events.\n",
    "\n",
    "prior probability = desired outcome/ total number of outcomes\n",
    "\n",
    "example: getting a head in tossing a coin(1/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240d6795-e866-43ad-ad7d-1269d7f45986",
   "metadata": {},
   "source": [
    "2. What is posterior probability? Give an example."
   ]
  },
  {
   "cell_type": "raw",
   "id": "1f7e78a5-674f-40e0-aca7-c8491f047e33",
   "metadata": {},
   "source": [
    "the posterior probability is defined as the probability of the outcome based on a given event that is already occurred. It is calculated by the baysen formula.\n",
    "  \n",
    "P(A∣B)= P(A)×P(B∣A)/P(B)\n",
    "\n",
    "we have 5 red balls and 10 black balls, choosing a red ball given that a red ball is chosen.(5/15*4/14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ec764a-cfe7-4fc2-ac16-a2313922eba7",
   "metadata": {},
   "source": [
    "3. What is likelihood probability? Give an example."
   ]
  },
  {
   "cell_type": "raw",
   "id": "c8dca5fc-3950-4ae1-a44c-d09116af0bfb",
   "metadata": {},
   "source": [
    "likelihood finds and determines the best data distribution of the model.\n",
    "\n",
    "we are tossing a coin, now head and tail have a 50-50 chance but when we toss a coin 50 times we get only 14 as a head. So likelihood determines this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5cb429-8d99-4094-9754-3f87836074e7",
   "metadata": {},
   "source": [
    "4. What is Naïve Bayes classifier? Why is it named so?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7c6ca366-143d-41aa-b414-e21c463ae34b",
   "metadata": {},
   "source": [
    "It's based on Bayes's theorem.\n",
    "\n",
    "P(A∣B)= P(A)×P(B∣A)/P(B)\n",
    "\n",
    "It's used for classification problems such as text classification, spam filters, etc.\n",
    "\n",
    "It's called naive because its feature is independent of others. It's called Bayes because it's based on Bayes theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a168d5d-208f-4911-baf8-72fdabf56c19",
   "metadata": {},
   "source": [
    "5. What is optimal Bayes classifier?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7f23bc0d-c9c5-4bf1-a89f-30383da3deb8",
   "metadata": {},
   "source": [
    "It uses Maximum a Posteriori(MAP) to find the best likely hypothesis from the taken ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28aca59-3523-4891-9283-56a9b6b2cac4",
   "metadata": {},
   "source": [
    "6. Write any two features of Bayesian learning methods."
   ]
  },
  {
   "cell_type": "raw",
   "id": "31c3a46c-c294-4100-a2da-545f765d8d40",
   "metadata": {},
   "source": [
    "Multiple hypotheses can be combined with probability to make best classification.\n",
    "\n",
    "Prior knowledge can be combined with observed data to determine the final probability of a hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a731f8-230b-4a91-adce-af83cee163db",
   "metadata": {},
   "source": [
    "7. Define the concept of consistent learners."
   ]
  },
  {
   "cell_type": "raw",
   "id": "c213ea0a-6c8b-4b1c-8a21-1031c2ff24bc",
   "metadata": {},
   "source": [
    "learner L with hypothesis H and data D is a consistent learner if it provided zero error when D is used in such H."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9423235-9dac-4d84-bb11-e43633103c26",
   "metadata": {},
   "source": [
    "8. Write any two strengths of Bayes classifier."
   ]
  },
  {
   "cell_type": "raw",
   "id": "e0086e05-81ab-444c-b525-8fbf8c501160",
   "metadata": {},
   "source": [
    "Multiple hypotheses can be combined with probability to make best classification.\n",
    "\n",
    "Prior knowledge can be combined with observed data to determine the final probability of a hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d052c18-7f89-4e6f-88f7-4e222f41b0cc",
   "metadata": {},
   "source": [
    "9. Write any two weaknesses of Bayes classifier."
   ]
  },
  {
   "cell_type": "raw",
   "id": "0ff2b7fa-949f-4875-b5fe-9472960a09aa",
   "metadata": {},
   "source": [
    "Initial knowledge of many probabilities is required.\n",
    "\n",
    "Very high computational cost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6c7db3-24d6-488c-8707-4574ac171a78",
   "metadata": {},
   "source": [
    "10. Explain how Naïve Bayes classifier is used for"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ab8598-bfdd-4151-a1b1-720c8a38a848",
   "metadata": {},
   "source": [
    "1. Text classification"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e0b98b9f-7758-4fc2-a774-36ed5f2dc2d1",
   "metadata": {},
   "source": [
    "First, we count words in the wise class-0 and class-1, let's say the word hello how many times repeated in the class-0 and class-1 dataset.\n",
    "Then we find the total probability of class-0 and class-1.\n",
    "Then sentence wise we will find probability using the bayes formula.\n",
    "Let's say the word is \"Hello friends\" so we will find hello, friends probability given class-0, and total probability of class-0.\n",
    "In the same way, we will find it respect to class-1. And it will be classified based on probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaea0d09-4dc5-4f69-883e-5915b466b698",
   "metadata": {},
   "source": [
    "2. Spam filtering"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5b3b0557-2c1f-4b0d-8485-d2edaca352c1",
   "metadata": {},
   "source": [
    "First, we count words in the wise spam and not-spam class, let's say the word hello how many times repeated in the spam and not spam dataset.\n",
    "Then we find the total probability of spam and not spam.\n",
    "Then sentence wise we will find probability using the bayes formula.\n",
    "Let's say the word is \"Hello friends\" so we will find hello, friends probability given spam, and total probability of spam.\n",
    "In the same way, we will find it respect to not-spam. And it will be classified based on probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65130baa-6a8a-4883-b167-bab6dc130e08",
   "metadata": {},
   "source": [
    "3. Market sentiment analysis"
   ]
  },
  {
   "cell_type": "raw",
   "id": "574c046f-3e34-4096-b944-ebfd79eb918e",
   "metadata": {},
   "source": [
    "First, we count words in the wise market-up and market-down class, let's say the word politicts how many times repeated in the market-up and market-down dataset.\n",
    "Then we find the total probability of market-up and market-down\n",
    "Then sentence wise we will find probability using the bayes formula.\n",
    "Let's say the word is \"election results\" so we will find election, results probability given market-up, and total probability of market-up.\n",
    "In the same way, we will find it respect to market-down. And it will be classified based on probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450b124f-7c40-4743-ac3a-2825368ca009",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
